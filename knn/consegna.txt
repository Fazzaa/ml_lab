Cosa fa il notebook:
1) fa il grafico dell'errore di classificazione che k-nn commette su Iris dataset con un crescente valore di k

2) trova il valore ottimale di k

3) Analizza l'effetto della pesatura dei voti dei vicini con la distanza dei vicini:

4) fa il grafico dell'errore di classificazione con la pesatura dei voti in base alle distanze dei vicini  

5) trova il valore ottimale di k

Cosa dovete fare voi:
1) mostrare lo scatter plot (in 2D, scegliendo 2 delle 4 features) dei dati di Iris, 
con un colore determinato dalla classe (colore rosso per Setosa, blu per Versicolor, verde per Virginica). 
Quale sono secondo voi le 2 feature migliori da usare?

2) mostrare uno scatter plot simile, ma ora il colore dei punti e` determinato dalla classe stimata da k-nn, 
con un valore scelto di k a vostra scelta.

3) Visualizzate il ROC plot del migliore albero di decisione che avete addestrato nell'esercizio n.1 

4) Confrontare gli alberi decisione e k-nn sullo spazio ROC: per quali valori di (TPR,FPR) k-nn e` meglio degli 
alberi di decisione?

5) Eseguite k-nn ma ora usate come funzione di distanza una funzione:

distance(x,y)= 1- k(x,y) 

dove k(x,y) e` un Kernel Gaussian-like  k(x,y) (per k(x,y) usate la Radial Basis Function 
con il parametro Gamma  = 1/sigma^2) che controlla la sua ampiezza.

Il parametro gamma deve essere aggiustato (tuned) al valore ottimale, a secondo 
dell'accuratezza del k-nn (in modo simile a quanto abbiamo fatto precedentemente per il parametro n_neighbors).
 In questo caso, scegliete un valore di k=7.